% Encoding: UTF-8

@InCollection{AlexNet,
  author    = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  title     = {{ImageNet} Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25},
  publisher = {Curran Associates, Inc.},
  year      = {2012},
  pages     = {1097--1105},
}

@article{bunyamin2016comparison,
	title={A comparison of retweet prediction approaches: the superiority of Random Forest learning method},
	author={Bunyamin, Hendra and Tunys, Tomas},
	journal={TELKOMNIKA (Telecommunication Computing Electronics and Control)},
	volume={14},
	number={3},
	pages={1052--1058},
	year={2016}
}

@INPROCEEDINGS{bunyamin2017automatic,  
	author={H. {Bunyamin} and L. {Sulistiani}},  
	booktitle={2017 21st International Computer Science and Engineering Conference (ICSEC)},   
	title={Automatic Topic Clustering Using Latent Dirichlet Allocation with Skip-Gram Model on Final Project Abstracts},   year={2017},  
	volume={},  
	number={},  
	pages={1-5},  
	doi={10.1109/ICSEC.2017.8443795}}
		
@ARTICLE{bunyamin2019topic,
		AUTHOR  = {Bunyamin, Hendra and Heriyanto and Novianti, Stevani and Sulistiani, Lisan}, 
		TITLE   = {Topic Clustering and Classification on Final Project Reports: a Comparison of Traditional and Modern Approaches},
		JOURNAL = "IAENG International Journal of Computer Science",
		YEAR    = {2019},
		month   = aug,
		VOLUME  = {46}, 
		NUMBER  = {3},
		PAGES   = {506-511}
	}
	
@article{Bunyamin_2021,
	doi = {10.1088/1757-899X/1077/1/012014},
	url = {https://dx.doi.org/10.1088/1757-899X/1077/1/012014},
	year = {2021},
	month = {feb},
	publisher = {IOP Publishing},
	volume = {1077},
	number = {1},
	pages = {012014},
	author = {Hendra Bunyamin and  Meyliana},
	title = {Classical and Deep Learning Time Series Prediction Techniques in the Case of Indonesian Economic Growth},
	journal = {IOP Conference Series: Materials Science and Engineering},
	abstract = {Gross Domestic Growth (GDP) as a proxy of an economic growth in one country is defined as the market value of services and goods produced by the country for a year. As the GDP of one country gets higher and higher through the years, this indicator points out that in general the economic growth of the country is growing; specifically, the values of services and goods accumulated in the market are increasing. To determine what indicators of a country that affects one’s economic growth remains an open question. Therefore, this research attempts to study those indicators and particularly utilize them to predict the economic growth. To answer those questions, this research employs diverse time series techniques ranging from classic time series analysis to machine learning and deep learning. Subsequently, our dataset comprises World Development Indicators (WDI) of Indonesia from 1962 to 2016. By measuring Root-Mean-Square Error (RMSE), we show Seasonal Autoregressive Integrated Average (SARIMA) and Convolutional LSTM give the the best performance from classical and deep learning techniques respectively. Our analysis shows SARIMA’s performance is boosted by its ability to capture trend and seasonality of the dataset; equally important, Convolutional LSTM equipped with convolutions as part of reading input into LSTM units significantly boost the performance over either Convolutional or LSTM networks. Furthermore, our analysis points out that the indicator which most strongly contribute to predict GDP is CO&lt;sub&gt;2&lt;/sub&gt; emission. This result agrees to the fact that some countries with high CO&lt;sub&gt;2&lt;/sub&gt; emission also has high GDP as well; also this finding should warn Indonesian government of the increasing CO&lt;sub&gt;2&lt;/sub&gt; pollution.}
}

@article{bunyamin2021utilizing,
	author = {Hendra Bunyamin},
	title = {Utilizing Indonesian Universal Language Model Fine-tuning for Text Classification},
	journal = {Journal of Information Technology and Computer Science},
	volume = {5},
	number = {3},
	year = {2021},
	keywords = {},
	abstract = {Inductive transfer learning technique has made a huge impact on the computer vision field. Particularly, computer vision  applications including object detection, classification, and segmentation, are rarely trained from scratch; instead, they are fine-tuned from pretrained models, which are products of learning from huge datasets. In contrast to computer vision, state-of-the-art natural language processing models are still generally trained from the ground up. Accordingly, this research attempts to investigate an adoption of the transfer learning technique for natural language processing. Specifically, we utilize a transfer learning technique called Universal Language Model Fine-tuning (ULMFiT) for doing an Indonesian news text classification task. The dataset for constructing the language model is collected from several news providers from January to December 2017 whereas the dataset employed for text classification task comes from news articles provided by the Agency for the Assessment and Application of Technology (BPPT). To examine the impact of ULMFiT, we provide a baseline that is a vanilla neural network with two hidden layers. Although the performance of ULMFiT on validation set is lower than the one of our baseline, we find that the benefits of ULMFiT for the classification task significantly reduce the overfitting, that is the difference between train and validation accuracies from 4% to nearly zero.},
	issn = {2540-9824},	pages = {325--337},
	doi = {10.25126/jitecs.202053215},
	url = {http://jitecs.ub.ac.id/index.php/jitecs/article/view/215}
}

@conference{ice-tes22,
	author={Hendra Bunyamin. and Hapnes Toba. and Meyliana. and Roro Wahyudianingsih.},
	title={Breast Cancer Histopathological Image Classification using Progressive Resizing Approach},
	booktitle={Proceedings of the 1st International Conference on Emerging Issues in Technology, Engineering and Science - ICE-TES,},
	year={2022},
	pages={351-357},
	publisher={SciTePress},
	organization={INSTICC},
	doi={10.5220/0010754100003113},
	isbn={978-989-758-601-9},
}

@article{toba2023masking,
	author = {Hapnes Toba and Hendra Bunyamin and Juan E Widyaya and Christian Wibisono and Lucky S Haryadi},
	title = {Masking preprocessing in transfer learning for damage building detection},
	journal = {IAES International Journal of Artificial Intelligence (IJ-AI)},
	volume = {12},
	number = {2},
	year = {2023},
	doi = {http://doi.org/10.11591/ijai.v12.i2.pp552-559},
	url = {https://ijai.iaescore.com/index.php/IJAI/article/view/21525}
}

@inproceedings{hofmann1999probabilistic,
	title={Probabilistic latent semantic indexing},
	author={Hofmann, Thomas},
	booktitle={Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval},
	pages={50--57},
	year={1999}
}

@inproceedings{howard-ruder-2018-universal,
	title = "Universal Language Model Fine-tuning for Text Classification",
	author = "Howard, Jeremy  and
	Ruder, Sebastian",
	booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P18-1031",
	doi = "10.18653/v1/P18-1031",
	pages = "328--339",
	abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}

@article{vahadane2016structure,
	title={Structure-preserving color normalization and sparse stain separation for histological images},
	author={Vahadane, Abhishek and Peng, Tingying and Sethi, Amit and Albarqouni, Shadi and Wang, Lichao and Baust, Maximilian and Steiger, Katja and Schlitter, Anna Melissa and Esposito, Irene and Navab, Nassir},
	journal={IEEE transactions on medical imaging},
	volume={35},
	number={8},
	pages={1962--1971},
	year={2016},
	publisher={IEEE}
}




@Comment{jabref-meta: databaseType:bibtex;}
